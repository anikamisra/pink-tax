{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multimodal clustering \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>image_url</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Arrival 5\" Shorts</td>\n",
       "      <td>https://cdn.shopify.com/s/files/1/0156/6146/fi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012 Lifting Hoodie</td>\n",
       "      <td>https://cdn.shopify.com/s/files/1/0156/6146/fi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Minimal Sports Bra</td>\n",
       "      <td>https://cdn.shopify.com/s/files/1/0156/6146/fi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Strength Department Graphic Joggers</td>\n",
       "      <td>https://cdn.shopify.com/s/files/1/0156/6146/fi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Arrival Shorts</td>\n",
       "      <td>https://cdn.shopify.com/s/files/1/0156/6146/pr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 title  \\\n",
       "0                    Arrival 5\" Shorts   \n",
       "1                  2012 Lifting Hoodie   \n",
       "2                   Minimal Sports Bra   \n",
       "3  Strength Department Graphic Joggers   \n",
       "4                       Arrival Shorts   \n",
       "\n",
       "                                           image_url  gender  \n",
       "0  https://cdn.shopify.com/s/files/1/0156/6146/fi...       1  \n",
       "1  https://cdn.shopify.com/s/files/1/0156/6146/fi...       1  \n",
       "2  https://cdn.shopify.com/s/files/1/0156/6146/fi...       0  \n",
       "3  https://cdn.shopify.com/s/files/1/0156/6146/fi...       0  \n",
       "4  https://cdn.shopify.com/s/files/1/0156/6146/pr...       1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"\" # your path here \n",
    "\n",
    "import pandas as pd \n",
    "data = pd.read_csv(f\"{path}gymshark_data.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above dataframe, I used the image urls downloaded each image to my local device, and then I made a new dataframe that contained the paths of all these images. However, seeing that I am not uploading the images to github, and the fact that the dataframe contains personal information, I have not included it. Please contact me if you need code on how to create this dataframe yourself. \n",
    "\n",
    "Basically you have to go through the above dataframe, download each image using the image_url, and then create a new column with the path to each respective image. I called this new dataframe \"gymshark_data_with_image_paths.csv\". \n",
    "\n",
    "code available here: https://docs.google.com/document/d/1Ocl57rQBJn0R4koIy3iB2aprEJbWyrlORK01-zob6JQ/edit?tab=t.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(f\"{path}/gymshark_data_with_image_paths.csv\")\n",
    "data = data.drop(\"image\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means clustering \n",
    "### 1. Early fusion with K-means clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try k-means, early fusion, semantic segmentation \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torchvision import transforms\n",
    "from sklearn.cluster import KMeans\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the text descriptions \n",
    "\n",
    "# we use small / basic bert model because we don't need anything too complex. \n",
    "# we use uncased because capitalization doesn't matter for our task. \n",
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "#model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def get_text_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=50)\n",
    "    outputs = model(**inputs)\n",
    "    # return embeddings as a single flattened vector \n",
    "    return outputs.last_hidden_state.mean(dim=1).detach().numpy().flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "with open('bert_model.pkl', 'wb') as f:\n",
    "    pickle.dump({'tokenizer': tokenizer, 'model': model}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bert_model.pkl', 'rb') as f:\n",
    "    saved_data = pickle.load(f)\n",
    "    tokenizer = saved_data['tokenizer']\n",
    "    model = saved_data['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crop out faces (these reveal the gender!)\n",
    "import cv2\n",
    "\n",
    "def crop_face(image_path): \n",
    "    image = cv2.imread(image_path)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_alt2.xml') \n",
    "    faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "    for (x, y, w, h) in faces:\n",
    "        image[y:y+h, x:x+w] = (0, 0, 0)\n",
    "    return image \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),   \n",
    "    transforms.ToTensor(),           \n",
    "    transforms.Normalize(           \n",
    "        mean=[0.485, 0.456, 0.406], \n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    ),\n",
    "])\n",
    "\n",
    "# image is in cv2.imread format \n",
    "def get_image_embedding(image):\n",
    "    #image = Image.open(image_path)\n",
    "    image = image_transform(image).unsqueeze(0)  \n",
    "    with torch.no_grad():\n",
    "        embedding = resnet_model(image)  \n",
    "    return embedding.flatten().numpy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import pickle \\n\\nwith open('resnet_model.pkl', 'wb') as f:\\n    pickle.dump(resnet_model, f)\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle \n",
    "\n",
    "with open('resnet_model.pkl', 'wb') as f:\n",
    "    pickle.dump(resnet_model, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('resnet_model.pkl', 'rb') as f:\n",
    "    resnet_model = pickle.load(f)\n",
    "\n",
    "# ensure the loaded model is in evaluation mode\n",
    "resnet_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "texts = data['title']\n",
    "images = data['image_path']\n",
    "\n",
    "import cv2 \n",
    "\n",
    "face_cascade = cv2.CascadeClassifier(f'{path}gymshark/cascades/haarcascade_frontalface_alt2.xml')\n",
    "\n",
    "def crop_face(image_path): \n",
    "    image = cv2.imread(image_path)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "    height, width = image.shape[:2]\n",
    "    for (x, y, w, h) in faces:\n",
    "        top = max(y - 70, 0)\n",
    "        bottom = min(y + h + 70, height)\n",
    "        left = max(x - 70, 0)\n",
    "        right = min(x + w + 70, width)\n",
    "        image[top:bottom, left:right] = (0, 0, 0)\n",
    "    return image \n",
    "\n",
    "\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),   \n",
    "    transforms.ToTensor(),          \n",
    "    transforms.Normalize(            \n",
    "        mean=[0.485, 0.456, 0.406], \n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    ),\n",
    "])\n",
    "\n",
    "# image is in cv2.imread format \n",
    "def get_image_embedding(image):\n",
    "    #image = Image.open(image_path)\n",
    "    # Convert BGR image (from cv2) to RGB for PIL compatibility\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = Image.fromarray(image)\n",
    "    image = image_transform(image).unsqueeze(0)  # Apply transformation and add batch dimension\n",
    "    with torch.no_grad():\n",
    "        embedding = resnet_model(image)  # Forward pass to get the feature vector\n",
    "    return embedding.flatten().numpy()  # Flatten the tensor to a 1D array\n",
    "\n",
    "\n",
    "for i, text in enumerate(texts):\n",
    "    text_embedding = get_text_embedding(text)\n",
    "    face_cropped_image = crop_face(images[i])\n",
    "    image_embedding = get_image_embedding(face_cropped_image)\n",
    "    combined_embedding = np.concatenate([text_embedding, image_embedding])\n",
    "    embeddings.append(combined_embedding)\n",
    "\n",
    "with open('embeddings.pkl', 'wb') as f: \n",
    "    pickle.dump(embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np \n",
    "with open('embeddings.pkl', 'rb') as f:\n",
    "    embeddings_loaded = pickle.load(f)\n",
    "\n",
    "# ensure it's the proper format before applying k-means clustering! \n",
    "embeddings_loaded = np.array(embeddings_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TESTING OUT IMAGE CROPPING \n",
    "image = cv2.imread(data.loc[2,'image_path'])\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "height, width = image.shape[:2]\n",
    "for (x, y, w, h) in faces:\n",
    "    top = max(y - 70, 0)\n",
    "    bottom = min(y + h + 70, height)\n",
    "    left = max(x - 70, 0)\n",
    "    right = min(x + w + 70, width)\n",
    "    image[top:bottom, left:right] = (0, 0, 0)\n",
    "cv2.imwrite(\"image_2_cropped.jpg\", image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply KMeans clustering with k=9\n",
    "cluster_num = 6\n",
    "kmeans = KMeans(n_clusters=6, random_state=42)\n",
    "kmeans.fit(embeddings_loaded)\n",
    "\n",
    "# Get the cluster labels\n",
    "cluster_labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"gymshark_data_with_image_paths.csv\")\n",
    "data = data.rename(columns={\"title\":\"Product\", \"image_url\":\"image_url\", \"gender\":\"Gender\"})\n",
    "data = data.drop(\"image\", axis=1)\n",
    "embeddings = []\n",
    "texts = data['Product']\n",
    "images = data['image_path']\n",
    "\n",
    "\n",
    "data['cluster'] = None \n",
    "for i, text in enumerate(texts):  \n",
    "    data.loc[i, 'cluster'] = cluster_labels[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 1 Sample:\n",
      "Strength Department Oversized Hoodie 0\n",
      "Training T-Shirt 1\n",
      "Arrival Regular Fit T-Shirt 1\n",
      "Crest Oversized Hoodie 1\n",
      "Training Oversized T-Shirt 0\n",
      "\n",
      "Cluster 2 Sample:\n",
      "Minimal Sports Bra 0\n",
      "Adapt Safari Seamless Bralette 0\n",
      "Minimal Sports Bra 0\n",
      "Ruched Sports Bra 0\n",
      "Cotton Lifting Sports Bra 0\n",
      "\n",
      "Cluster 3 Sample:\n",
      "Training Leggings 0\n",
      "Training Fleece Joggers 0\n",
      "Vital Seamless 2.0 Leggings 0\n",
      "Vital Seamless 2.0 Leggings 0\n",
      "2012 Lifting Joggers 1\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "grouped = data.groupby('cluster')\n",
    "# change this depending on number of clusters \n",
    "for cluster_number in [1,2, 3]:\n",
    "    cluster_data = grouped.get_group(cluster_number)\n",
    "    if len(cluster_data) >= 5: \n",
    "        sample = cluster_data.sample(n=5)\n",
    "    else:\n",
    "        sample = cluster_data\n",
    "    \n",
    "    print(f\"\\nCluster {cluster_number} Sample:\")\n",
    "    for _, row in sample.iterrows():\n",
    "        print(row['title'], row['gender'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 4 Sample:\n",
      "Legacy T-Shirt 1\n",
      "Strength Department Brushed Sweatshirt 0\n",
      "Heritage Washed Hoodie 1\n",
      "Strength Department Brushed Sweatshirt 0\n",
      "Power T-Shirt 1\n",
      "\n",
      "Cluster 5 Sample:\n",
      "Arrival Shorts 1\n",
      "Arrival 5\" Shorts 1\n",
      "Arrival 5\" Shorts 1\n",
      "Everyday Seamless Shorts 0\n",
      "Adapt Safari Tight Shorts 0\n"
     ]
    }
   ],
   "source": [
    "for cluster_number in [4, 5]:\n",
    "    cluster_data = grouped.get_group(cluster_number)\n",
    "    if len(cluster_data) >= 5: \n",
    "        sample = cluster_data.sample(n=5)\n",
    "    else:\n",
    "        sample = cluster_data\n",
    "    \n",
    "    print(f\"\\nCluster {cluster_number} Sample:\")\n",
    "    for _, row in sample.iterrows():\n",
    "        print(row['title'], row['gender']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking pretty good! these clusters seem to resemble cohesive clothing categories such as shirts, shorts, sweaters, bras, and pants!! \n",
    "\n",
    "### 2. Late fusion with K means clustering \n",
    "\n",
    "https://ieeexplore.ieee.org/document/8673554 \n",
    "experiment with different weights for late fusion \n",
    "\n",
    "https://ieeexplore.ieee.org/abstract/document/10185035 \n",
    "states that early fusion should work better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate each embedding from image and text esp if you don't want to run all that again \n",
    "\n",
    "text_embeddings = []\n",
    "image_embeddings = []\n",
    "\n",
    "# assuming positive numbers \n",
    "def round_normal(floatie): \n",
    "    return (int(floatie)+1) if floatie - int(floatie) >= 0.5 else int(floatie)\n",
    "\n",
    "i = 0\n",
    "while i < len(embeddings_loaded): \n",
    "    current_embedding = embeddings_loaded[i]\n",
    "    text_embeddings.append(np.array(current_embedding[:768]))\n",
    "    image_embeddings.append(np.array(current_embedding[768:]))\n",
    "    i = i+1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run k-means clustering on each \n",
    "\n",
    "# you can edit these params \n",
    "cluster_num = 6\n",
    "weight_for_image = 0.99 #weight for text automatically set to 1-(weight for image)\n",
    "weight_for_text = 1-weight_for_image\n",
    "\n",
    "kmeans_text = KMeans(n_clusters=cluster_num, random_state=42)\n",
    "kmeans_text.fit(text_embeddings)\n",
    "cluster_label_text =  kmeans_text.labels_\n",
    "\n",
    "kmeans_images = KMeans(n_clusters=cluster_num, random_state=42)\n",
    "kmeans_images.fit(image_embeddings)\n",
    "cluster_label_images = kmeans_images.labels_\n",
    "\n",
    "data['late_fusion_cluster'] = None\n",
    "\n",
    "i = 0 \n",
    "for cluster_pair in zip(cluster_label_text, cluster_label_images): \n",
    "    modified_cluster_val = (weight_for_image)*cluster_label_images[i] + (weight_for_text)*cluster_label_text[i]\n",
    "    data.loc[i, 'late_fusion_cluster'] = round_normal(modified_cluster_val)\n",
    "    i = i+1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 1 Sample:\n",
      "Strength Department Oversized Hoodie 0\n",
      "GS Power Oversized T-Shirt 0\n",
      "Arrival Regular Fit T-Shirt 1\n",
      "Impact Drop Arm Tank 1\n",
      "Lifting Essentials Oversized Sweatshirt 0\n",
      "\n",
      "Cluster 2 Sample:\n",
      "Minimal Sports Bra 0\n",
      "Everyday Seamless Sports Bra 0\n",
      "One Shoulder Sports Bra 0\n",
      "Minimal Sports Bra 0\n",
      "Everyday Seamless Sports Bra 0\n",
      "\n",
      "Cluster 3 Sample:\n",
      "2012 Lifting Joggers 1\n",
      "Vital Seamless 2.0 Leggings 0\n",
      "Adapt Camo Seamless Leggings 0\n",
      "Everyday Seamless Leggings 0\n",
      "Elevate Leggings 0\n"
     ]
    }
   ],
   "source": [
    "grouped = data.groupby('late_fusion_cluster')\n",
    "for cluster_number in [1,2,3]:\n",
    "    cluster_data = grouped.get_group(cluster_number)\n",
    "    if len(cluster_data) >= 5: \n",
    "        sample = cluster_data.sample(n=5)\n",
    "    else:\n",
    "        sample = cluster_data\n",
    "    \n",
    "    print(f\"\\nCluster {cluster_number} Sample:\")\n",
    "    for _, row in sample.iterrows():\n",
    "        print(row['title'], row['gender'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5]\n",
      "\n",
      "Cluster 4 Sample:\n",
      "Rest Day Essential Crew 1\n",
      "Training Oversized Fleece Sweatshirt 0\n",
      "Crest Hoodie 1\n",
      "Rest Day Essential Crew 1\n",
      "Rest Day Essential Crew 1\n",
      "\n",
      "Cluster 5 Sample:\n",
      "Graphic Crew Socks 7PK 0\n",
      "Graphic Crew Socks 7PK 0\n",
      "Graphic Crew Socks 7PK 0\n",
      "Graphic Crew Socks 7PK 0\n",
      "Graphic Crew Socks 7PK 0\n"
     ]
    }
   ],
   "source": [
    "print(data['late_fusion_cluster'].unique())\n",
    "for cluster_number in [4, 5]:\n",
    "    cluster_data = grouped.get_group(cluster_number)\n",
    "    if len(cluster_data) >= 5: \n",
    "        sample = cluster_data.sample(n=5)\n",
    "    else:\n",
    "        sample = cluster_data\n",
    "    \n",
    "    print(f\"\\nCluster {cluster_number} Sample:\")\n",
    "    for _, row in sample.iterrows():\n",
    "        print(row['title'], row['gender']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These clusters seem okay too. Theoretiically tho early fusion is better for my type of problem (since we don't have missing data) so I think i'll just stick with the results of early fusion. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FINAL PRICING ANALYSIS \n",
    "\n",
    "Let's use the clusters determined by early fusion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import statistics\n",
    "\n",
    "def round_normal(floatie): \n",
    "    return (int(floatie)+1) if floatie - int(floatie) >= 0.5 else int(floatie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_df = pd.read_csv(\"gymshark_data_with_prices.csv\")\n",
    "price_df_dict = dict(\n",
    "    zip(\n",
    "        zip(price_df)\n",
    "    )\n",
    "\n",
    ")\n",
    "grouped = price_df.groupby([\"Product\", \"Gender\"])[\"Price\"].mean()\n",
    "price_dict = grouped.to_dict()\n",
    "grouped2 = data.groupby([\"Product\", \"Gender\"])[\"cluster\"].mean()\n",
    "data_dict = grouped2.to_dict()\n",
    "price_a = pd.DataFrame(columns=[\"Cluster\", \"Average womens price\", \"Average mens price\"])\n",
    "\n",
    "# change number of lists based on how many clusters you ran \n",
    "cluster1_w = []\n",
    "cluster1_m = []\n",
    "cluster2_w = []\n",
    "cluster2_m = []\n",
    "cluster3_w = []\n",
    "cluster3_m = []\n",
    "cluster4_w = []\n",
    "cluster4_m = []\n",
    "cluster5_w = []\n",
    "cluster5_m = []\n",
    "\n",
    "\n",
    "cluster_dict = {f\"cluster{i}_{gender}\": [] for i in range(0, 7) for gender in ['w', 'm']}\n",
    "\n",
    "for product_tuple, price in price_dict.items(): \n",
    "    if product_tuple in data_dict.keys(): \n",
    "        cluster = round_normal(data_dict[product_tuple])\n",
    "        gender = 'w' if product_tuple[1] == 0 else 'm'\n",
    "        list_name = f\"cluster{cluster}_{gender}\"\n",
    "        cluster_dict[list_name].append(price)\n",
    "\n",
    "i = 0 \n",
    "for cat_name, prices in cluster_dict.items(): \n",
    "    cluster = cat_name[7]\n",
    "    gender = \"womens\" if cat_name[9] == 0 else \"mens\"\n",
    "    avg_price = statistics.mean(prices)\n",
    "    if gender == 0: \n",
    "        price_a.loc[i] = [cluster, ]\n",
    "cluster_list = list(cluster_dict.items())\n",
    "i = 0 \n",
    "while i < len(cluster_list): \n",
    "    if i % 2 == 1: \n",
    "        cluster_name = cluster_list[i][0]\n",
    "        cluster = cluster_name[7]\n",
    "        list_of_prices = cluster_list[i][1]\n",
    "        avg_price_m = statistics.mean(list_of_prices) if len(list_of_prices) > 0 else 0 \n",
    "        index = (i-1)/2\n",
    "        price_a.loc[index] = [cluster, avg_price_w, avg_price_m]\n",
    "        avg_price_m = '20000' \n",
    "        avg_price_w = '20000'\n",
    "    if i % 2 == 0: \n",
    "        list_of_prices = cluster_list[i][1]\n",
    "        avg_price_w = statistics.mean(list_of_prices) if len(list_of_prices) > 0 else 0 \n",
    "    i+=1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Cluster  Average womens price  Average mens price\n",
      "0.0        0             32.000000            0.000000\n",
      "1.0        1             39.373333           27.851832\n",
      "2.0        2             30.500000            0.000000\n",
      "3.0        3             48.325000           43.529167\n",
      "9.0        4             37.744000           37.928571\n",
      "5.0        5             40.000000           24.950000\n",
      "13.0       6              0.000000            0.000000\n"
     ]
    }
   ],
   "source": [
    "price_a = price_a.drop_duplicates(subset=[\"Cluster\"])\n",
    "price_a = price_a.sort_values(by='Cluster', ascending=True)\n",
    "print(price_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# And voila! \n",
    "After lots of experimenting, we have finally created an automated approach to \n",
    "1. Creating cohesive and meaningful clothing categories across gender (5-6 clusters is a good number)\n",
    "2. Comparing the price across categories to quantify any pricing discrimination! \n",
    "\n",
    "Looks like  multimodal AI saved the day. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
