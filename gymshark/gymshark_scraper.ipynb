{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Womens clothing formatting\n",
    "\n",
    "Luckily, all womens products are neatly displaced into one section of the website. \n",
    "However, there are numerous pages you have to go through.\n",
    "Once again, these pages pretty much all have the same url format, except for the first page. See below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_page_w = \"https://www.gymshark.com/collections/all-products/womens\"\n",
    "\n",
    "# all other 16 pages have this format \n",
    "# page_number ranges from 1 to 16 (inclusive)\n",
    "#page_number_w = \"1\"\n",
    "url_format_w = \"https://www.gymshark.com/collections/all-products/womens?page=\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mens clothing formatting\n",
    "\n",
    "Same as before, except replace \"womens\" with \"mens\" in the urls. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_page_m = \"https://www.gymshark.com/collections/all-products/mens\"\n",
    "page_number_m = \"1\" #ranges from 1 to 16, inclusive \n",
    "url_format_m = \"https://www.gymshark.com/collections/all-products/mens?page=\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping the clothing \n",
    "\n",
    "First, import packages womens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anikamisra/miniconda3/envs/16A/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrape womens clothing\n",
    "\n",
    "Test run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_df = pd.DataFrame(columns=['Product', 'Price'])\n",
    "result = requests.get(url_format_w)\n",
    "soup = BeautifulSoup(result.text, 'html.parser')\n",
    "product_info_divs = soup.find_all('div', {'class': 'product-card_product-information___4MDP'})\n",
    "i = 1\n",
    "for div in product_info_divs:\n",
    "    product_title = div.find('h4', {'class': 'product-card_product-title__9gis1'}).text\n",
    "    product_price = div.find('span', {'class': 'product-card_product-price__bNBmg'}).text\n",
    "    w_df.loc[i] = [product_title, product_price]\n",
    "    i+= 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GS Power Joggers</td>\n",
       "      <td>$56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Midi 3pk Socks</td>\n",
       "      <td>$16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vital Seamless  2.0 Leggings</td>\n",
       "      <td>$54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Stripe Crew Single</td>\n",
       "      <td>$12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Rest Day Sweats Hoodie</td>\n",
       "      <td>$60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Product Price\n",
       "1              GS Power Joggers   $56\n",
       "2                Midi 3pk Socks   $16\n",
       "3  Vital Seamless  2.0 Leggings   $54\n",
       "4            Stripe Crew Single   $12\n",
       "5        Rest Day Sweats Hoodie   $60"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool. Now for the actual thing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.gymshark.com/collections/all-products/womens?page=1\n",
      "https://www.gymshark.com/collections/all-products/womens?page=2\n",
      "https://www.gymshark.com/collections/all-products/womens?page=3\n",
      "https://www.gymshark.com/collections/all-products/womens?page=4\n",
      "https://www.gymshark.com/collections/all-products/womens?page=5\n",
      "https://www.gymshark.com/collections/all-products/womens?page=6\n",
      "https://www.gymshark.com/collections/all-products/womens?page=7\n",
      "https://www.gymshark.com/collections/all-products/womens?page=8\n",
      "https://www.gymshark.com/collections/all-products/womens?page=9\n",
      "https://www.gymshark.com/collections/all-products/womens?page=10\n",
      "https://www.gymshark.com/collections/all-products/womens?page=11\n",
      "https://www.gymshark.com/collections/all-products/womens?page=12\n",
      "https://www.gymshark.com/collections/all-products/womens?page=13\n",
      "https://www.gymshark.com/collections/all-products/womens?page=14\n",
      "https://www.gymshark.com/collections/all-products/womens?page=15\n",
      "https://www.gymshark.com/collections/all-products/womens?page=16\n"
     ]
    }
   ],
   "source": [
    "w_df = pd.DataFrame(columns=['Product', 'Price'])\n",
    "\n",
    "# first page \n",
    "result = requests.get(first_page_w)\n",
    "soup = BeautifulSoup(result.text, 'html.parser')\n",
    "prod_inf_divs = soup.find_all('div', {'class': 'product-card_product-information___4MDP'})\n",
    "i = 1\n",
    "for div in prod_inf_divs:\n",
    "    product_title = div.find('h4', {'class': 'product-card_product-title__9gis1'}).text\n",
    "    product_price = div.find('span', {'class': 'product-card_product-price__bNBmg'}).text\n",
    "    w_df.loc[i] = [product_title, product_price]\n",
    "    i+= 1 \n",
    "w_df.head()\n",
    "\n",
    "# the rest of the 16 pages, since the url format is different \n",
    "PAGES_W = 16 \n",
    "page_number_w = 1\n",
    "i = page_number_w\n",
    "while page_number_w <= PAGES_W: \n",
    "    current_url = url_format_w + str(page_number_w)\n",
    "    print(current_url)\n",
    "    result = requests.get(current_url)\n",
    "    soup = BeautifulSoup(result.text, 'html.parser')\n",
    "    prod_info_divs = soup.find_all('div', {'class': 'product-card_product-information___4MDP'})\n",
    "    #i = page_number_w\n",
    "    for div in prod_inf_divs: \n",
    "        product_title = div.find('h4', {'class': 'product-card_product-title__9gis1'}).text\n",
    "        product_price = div.find('span', {'class': 'product-card_product-price__bNBmg'}).text\n",
    "        w_df.loc[i] = [product_title, product_price]\n",
    "        i = i+1 \n",
    "    \n",
    "    page_number_w += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "960"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_df.to_csv('w_df_scraped_jul_14.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! And now for mens, the same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.gymshark.com/collections/all-products/mens?page=1\n",
      "https://www.gymshark.com/collections/all-products/mens?page=2\n",
      "https://www.gymshark.com/collections/all-products/mens?page=3\n",
      "https://www.gymshark.com/collections/all-products/mens?page=4\n",
      "https://www.gymshark.com/collections/all-products/mens?page=5\n",
      "https://www.gymshark.com/collections/all-products/mens?page=6\n",
      "https://www.gymshark.com/collections/all-products/mens?page=7\n",
      "https://www.gymshark.com/collections/all-products/mens?page=8\n",
      "https://www.gymshark.com/collections/all-products/mens?page=9\n",
      "https://www.gymshark.com/collections/all-products/mens?page=10\n",
      "https://www.gymshark.com/collections/all-products/mens?page=11\n",
      "https://www.gymshark.com/collections/all-products/mens?page=12\n",
      "https://www.gymshark.com/collections/all-products/mens?page=13\n",
      "https://www.gymshark.com/collections/all-products/mens?page=14\n",
      "https://www.gymshark.com/collections/all-products/mens?page=15\n",
      "https://www.gymshark.com/collections/all-products/mens?page=16\n"
     ]
    }
   ],
   "source": [
    "m_df = pd.DataFrame(columns=['Product', 'Price'])\n",
    "\n",
    "# first page \n",
    "result = requests.get(first_page_m)\n",
    "soup = BeautifulSoup(result.text, 'html.parser')\n",
    "prod_inf_divs = soup.find_all('div', {'class': 'product-card_product-information___4MDP'})\n",
    "i = 1\n",
    "for div in prod_inf_divs:\n",
    "    product_title = div.find('h4', {'class': 'product-card_product-title__9gis1'}).text\n",
    "    product_price = div.find('span', {'class': 'product-card_product-price__bNBmg'}).text\n",
    "    m_df.loc[i] = [product_title, product_price]\n",
    "    i+= 1 \n",
    "\n",
    "# the rest of the 16 pages, since the url format is different \n",
    "PAGES_M = 16 \n",
    "page_number_m = 1\n",
    "i = page_number_m\n",
    "while page_number_m <= PAGES_M: \n",
    "    current_url = url_format_m + str(page_number_m)\n",
    "    print(current_url)\n",
    "    result = requests.get(current_url)\n",
    "    soup = BeautifulSoup(result.text, 'html.parser')\n",
    "    prod_info_divs = soup.find_all('div', {'class': 'product-card_product-information___4MDP'})\n",
    "    #i = page_number_w\n",
    "    for div in prod_inf_divs: \n",
    "        product_title = div.find('h4', {'class': 'product-card_product-title__9gis1'}).text\n",
    "        product_price = div.find('span', {'class': 'product-card_product-price__bNBmg'}).text\n",
    "        m_df.loc[i] = [product_title, product_price]\n",
    "        i = i+1 \n",
    "    \n",
    "    page_number_m += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "960"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_df.to_csv('m_df_scraped_jul_14.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-trained models / packages to label clothing that I can look into \n",
    "1. Google BERT\n",
    "-  RoBERTA\n",
    "- DistilBert\n",
    "2. ULMFiT\n",
    "3. GPT (or literally just chat gpt lol)\n",
    "4. XLNet\n",
    "\n",
    "Right now it's looking like ChatGPT is the best option because all the others require me labeling this entire dataset (since I need to train on it) which is the entire point of me using a package to label clothing. \n",
    "\n",
    "I could also use like a pre-made dataset but \n",
    "A) a dataset like this would be kind of hard to find (like, a dataset containing the name that the store chooses for a product, and then what the product actually is) \n",
    "B) even if I did find a dataset like this, I would need it to be eponymically similar to Garage's style of naming clothes. \n",
    "\n",
    "So if I just put in the clothing list, 100 at a time, into chat GPT (4o) and give it specific instructions to categorize my list into 8 major categories, then that should be fine. \n",
    "\n",
    "## Limitations of this approach \n",
    "Write here\n",
    "\n",
    "## Approach 2: hardcoding \n",
    "Skimming the data with my own two eyes, I see that the products are actually labelled very simply and I might be better off just hardcoding the labelling myself. Here are the categories I want to break them into: \n",
    "\n",
    "Limitations of this approach: depends on me skimming everything properly lol \n",
    "\n",
    "Womens: \n",
    "- sweatpants (\"Joggers\")\n",
    "- shorts (\"Shorts\")\n",
    "- tanks (\"tank\")\n",
    "- crop tops (\"crop top\")\n",
    "- t-shirts (\"T-shirt\")\n",
    "- other top (\"Top\", \"tee\", \"cami\", \"bandeau\")\n",
    "- bra (\"bra\")\n",
    "- socks (\"socks\")\n",
    "- skirt (\"skirt\" and \"skort\")\n",
    "- dress \n",
    "- other (this includes lifting straps, headbands, etc)\n",
    "\n",
    "Mens: \n",
    "- sweatpants (\"joggers\")\n",
    "- shorts \n",
    "- tanks \n",
    "- t-shirts \n",
    "- all other shirts \n",
    "- sweater (\"hoodie\", \"crew\", \"sweatshirt\", \"pullover\", \"jacket\")\n",
    "- socks \n",
    "- other "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "womens_df = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
